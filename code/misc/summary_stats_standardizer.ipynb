{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366bb2e-c586-4775-8493-f4b4c8ccc380",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429e458-7270-450a-898e-4de2dfd789e5",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "899742d1-5690-458a-9869-7086c0630828",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Summary statistics standardization and export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce9ed4e-2bd0-49ae-9c42-658cb9711cd4",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This pipeline module contains codes to process summary statistics from conventional QTL association scan to standard formats for public distribution. It will also export multiple QTL studies to formats easily accessible for data integration methods to query and analyze the summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-license",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview of design\n",
    "\n",
    "### Individual xQTL studies\n",
    "\n",
    "1. Reorganize the QTL marginal statistics to standard formats (see section `Column name standardization` for details).\n",
    "2. Report separately cis-QTL and trans-QTL. Additionally report filtered results, e.g. QTLs that survived multiple-testing correction.\n",
    "3. Our column conventions are based on studies such as GTEx and eQTL category. Our design is \"modular\" in the sense that we do not provide information that could be trivially annotated after, such as **rsID, gene symbol, gene biotype, gene start and end positions**; or for information that can be inferred from other columns such as type of allele (SNP or INDEL).\n",
    "4. We use GRCh38 reference allele and alternative allele, and the effect allele is adjusted, as necessary, to the alternative allele.\n",
    "5. Summary statistics will be in conventional TSV format which can be converted to [GWAS-VCF format](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02248-0) as needed.\n",
    "\n",
    "### Multiple xQTL integration\n",
    "\n",
    "- We will **NOT** standardize integrative results. Instead we will distribute the original outputs from the data integration methods we chose. \n",
    "- For meta-analysis we'll rely on output from [METASOFT](http://genetics.cs.ucla.edu/meta/) using `effect_id` as SNP ID, created from `variant`, `molecular_trait_id` and `molecular_trait_object_id` as needed.\n",
    "\n",
    "To build internal database for multiple xQTL data-sets, we:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-botswana",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. Include variants that presents in at least one xQTL. \n",
    "2. Unify allele strand and frequency flips to GRCh38 reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c32c06-80fd-45c5-8d0a-69a9464cdd5c",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Column name standardization\n",
    "\n",
    "The header of actual output sumstat depends on how we configure it (see section `Input` for details). However, all of them will have the column of `chromosome, position, ref, alt, variant_id, beta, se, pvalue`. \n",
    "\n",
    "### Software (input) headers\n",
    "\n",
    "For example, when the input sumstat is from TensorQTL, the column specification is:\n",
    "\n",
    "- GENE: Molecular trait identifier.(gene)\n",
    "- CHR: Variant chromosome.\n",
    "- POS: Variant chromosomal position (basepairs).\n",
    "- A0: Variant reference allele (A, C, T, or G).\n",
    "- A1: Variant alternate allele.\n",
    "- TSS_D: Distance of the SNP to the gene transcription start site (TSS)\n",
    "- AF: The allele frequency of this SNPs\n",
    "- MA_SAMPLES: Number of samples carrying the minor allele\n",
    "- MA_COUNT: Total number of minor alleles across individuals\n",
    "- P: Nominal P-value from linear regression\n",
    "- STAT: Slope of the linear regression\n",
    "- SE: Standard error of beta\n",
    "\n",
    "when the input sumstat is from APEX, the column specification is:\n",
    "\n",
    "- GENE: Molecular trait identifier.(gene)\n",
    "- CHR: Variant chromosome.\n",
    "- POS: Variant chromosomal position (basepairs).\n",
    "- A0: Variant reference allele (A, C, T, or G).\n",
    "- A1: Variant alternate allele.\n",
    "- P: Nominal P-value from linear regression\n",
    "- STAT: Slope of the linear regression\n",
    "- SE: Standard error of beta\n",
    "\n",
    "### Our effect level summary\n",
    "\n",
    "Our proposed xQTL summary statistics fields should include (cf. [our xQTL format draft V3, Jan 2021](https://www.niagads.org/adsp/content/xqtl-fileformats-110921v3sharedxlsx), [eQTL catalog](https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/tabix/Columns.md), [GTEx](https://www.gtexportal.org/home/datasets), [metaBrain](https://www.metabrain.nl/)):\n",
    "\n",
    "* **variant** - The variant ID (chromosome_position_ref_alt) e.g. chr19_226776_C_T. Based on GRCh38 coordinates and reference genome, with 'chr' prefix added to the chromosome number.\n",
    "* **chromosome** - GRCh38 chromosome name of the variant (e.g. 1,2,3 ...,X).\n",
    "* **position** - GRCh38 position of the variant.\n",
    "* **ref** - GRCh38 reference allele.\n",
    "* **alt** - GRCh38 alternative allele (also the effect allele).\n",
    "* **imputation_quality** - Optional imputation quality score from the imputation software, can be replaced with NA if not available.\n",
    "* **molecular_trait_object_id** - For phenotypes with multiple correlated alternatives (multiple alternative transcripts or exons within a gene, multple alternative promoters in txrevise, multiple alternative intons in Leafcutter), this defines the level at which the phenotypes were aggregated. Permutation p-values are calculated across this set of alternatives.  \n",
    "* **molecular_trait_id** - ID of the molecular trait used for QTL mapping. Depending on the quantification method used, this can be either a gene id, exon id, transcript id or a txrevise promoter, splicing or 3'end event id. Examples: ENST00000356937, ENSG00000008128.  \n",
    "* **maf** - Minor allele frequency within a QTL mapping context (e.g. cell type or tissues within a study).\n",
    "* **beta** - Regression coefficient from the linear model.\n",
    "* **se** - Standard error of the beta.\n",
    "* **pvalue** - Nominal p-value of association between the variant and the molecular trait.\n",
    "* **n** - Total number of samples without missing data.\n",
    "* **ac** - Count of the alternative allele. \n",
    "* **ma_samples** - Number of samples carrying at least one copy of the minor allele.\n",
    "\n",
    "### Trait level QTL summary (multiple-testing corrected)\n",
    "\n",
    "* **molecular_trait_object_id** \n",
    "* **molecular_trait_id** \n",
    "* **n_traits** - The number of molecular traits over which permutation p-values were calculated (e.g. the number of transcripts per gene). Note that the permutations are performed accross all molecular traits within the same molecular trait object (e.g. all transcripts of a gene) and the results are reported for the most significant variant and molecular trait pair. \n",
    "* **n_variants** - number of genetic variants tested within the cis region of the molecular trait.\n",
    "* **variant** \n",
    "* **chromosome** - GRCh38 chromosome name of the variant (e.g. 1,2,3 ...,X).\n",
    "* **position** - GRCh38 position of the variant.\n",
    "* **ref** - GRCh38 reference allele.\n",
    "* **alt** - GRCh38 alternative allele (also the effect allele).\n",
    "* **p_perm** - Empirical p-value calculated from 1000 permutations.\n",
    "* **p_beta** - Estimated empirical p-value based on the beta distribution. This is the column that you want to use for filtering the results. See the FastQTL [paper](http://dx.doi.org/10.1093/bioinformatics/btv722) for more details. \n",
    "* **qvalue** - FDR based on Storey's q-value.\n",
    "\n",
    "Other summary:\n",
    "\n",
    "- Quantiles of molecular phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-renewal",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Some technical notes\n",
    "\n",
    "1. If there are duplicated INDELs in the summary statistics, they will be removed. For example, two SNPs at 10000 on chr1. one's `A0` is `T`, and `A1` is `TC`. Whereas the other one's `A0` is `TC`, and `A1` is `T`. Both of them will be removed. More about INDEL issues(https://github.com/statgenetics/UKBB_GWAS_dev/issues/81#issuecomment-1015556800). For SNPs, `A0` and `A1` can be easily standardized to ref/alt in GRCh38 reference genome.\n",
    "2. If duplicated `chr:pos` (GWAS) or `gene:chr:pos` (TWAS) exist, run a recursive match for each pair of them between two summary statistic files (`query`(each of inputs) and `subject` (target file)). \n",
    "3. under the same `chr:pos` or `gene:chr:pos`, The variants' `A0` and `A1` are matched by exact, flip, reverse, or flip+reverse models. Only one of them is `True`, the variant in two files are matched. If they are matched by flip or flip+reverse, the sign of `query`'s `STAT` will be inversed. And the `query`'s `A0` and `A1` will be the same as the `subject`'s `A0` and `A1`.  **FIXME: should we standardize it to GRCh38 first?**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-concert",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Make sure you install the pre-requisited before running this notebook:\n",
    "\n",
    "```\n",
    "pip install cugg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-reconstruction",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "- `--cwd`, the path of working directory\n",
    "- `--yml_list`, the path to a list of yaml file\n",
    "-\n",
    "- `--keep-ambiguous`, boolean. default False. if add --keep-ambiguous parameter, keep ambiguous alleles which can not be decided from flip or reverse, such as A/T or C/G. Otherwise, remove them. \n",
    "- `--intersect`, boolean. default False. if add --intersect parameter, output intersect SNPs in all input files.\n",
    "- `--TARGET_list`a path to a list of reference file, with the column name CHR, POS, REF, ALT that represent the correct reference allele. If these reference is not availble, it can be generated by the TARGET_generation step of this workflow.\n",
    "\n",
    "- TARGET\n",
    "   - The target file is a reference summary statistic file or a file with at least variant ID relevant columns. When provided with standard `chr, pos, ref, alt` based on GRCh38, it can serve the purpose to standardize the REF/ALT alleles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-rapid",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### The minimal format of the input yaml file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-jurisdiction",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For GWAS summary statistics, \n",
    "\n",
    "```\n",
    "INPUT:\n",
    "  - ./data/testflip/*.gz:\n",
    "        build: GRCh38\n",
    "        variant: chromosome, position, ref, alt\n",
    "        chromosome: CHR\n",
    "        position: POS\n",
    "        ref: A0\n",
    "        alt: A1\n",
    "        beta: BETA\n",
    "        se: SE\n",
    "        pvalue: P\n",
    "  - ./data/testflip/flip/snps500_flip.regenie.snp_stats.gz:\n",
    "        build: GRCh38\n",
    "        variant: chromosome, position, ref, alt\n",
    "        chromosome: CHR\n",
    "        position: POS\n",
    "        ref: A0\n",
    "        alt: A1\n",
    "        beta: BETA\n",
    "        se: SE\n",
    "        pvalue: P\n",
    "        \n",
    "OUTPUT: data/testflip/output/\n",
    "```\n",
    "\n",
    "For xQTL summary statistics, `molecular_trait_object_id` is required because a variant can be made association with multiple molecular traits. \n",
    "\n",
    "```\n",
    "INPUT:\n",
    "  - data/twas/*.txt:\n",
    "        build: GRCh38\n",
    "        variant: chromosome, position, ref, alt\n",
    "        chromosome: CHR\n",
    "        position: POS\n",
    "        ref: A0\n",
    "        alt: A1\n",
    "        molecular_trait_id: GENE\n",
    "        beta: BETA\n",
    "        se: SE\n",
    "        pvalue: P \n",
    "\n",
    "OUTPUT: ../data/twas/output/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-hierarchy",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "There are three parts in the input yaml file.\n",
    "- INPUT\n",
    "   - A list of yml file, as the output from yml_generator, each yml file documents a set of input\n",
    "       - the input summary statistic files with the column names in below. \n",
    "       - the input files can be from multiple directory and from different format. The input paths must follow the rules related to Unix shell. the format is to pair the column names with required keys. If not provided, the column names of the input file will be considered as the default keys.\n",
    "       - The input summary statistic file cannot have duplicated chr:pos\n",
    "       - The input summary statstic file cannot have # in its header\n",
    "       -`variant` in yml is the rule to generate a unique identifier for each SNP, the content of variant ID shall be a combination of other columns such as chrom, position, ref, alt, build, but not taken from existing id columns in the original file.\n",
    "\n",
    "- OUTPUT\n",
    "   - the path of an output directory for new summary statistic files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe16026-54d2-4a56-9855-c681ae411d53",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "New summary statistic files with common SNPs in all input files. the sign of statistics has been corrected to make it consistent in different data.\n",
    "   - for each input sumstat file, a standardized version of it will be generated.\n",
    "   - The generated sumstat files will have header standardized header names. The minimal set of headers will be \\\"chromosome, position, ref, alt, variant_id, beta, se, pvalue\\\"\n",
    "   - The generated sumstat files will be in gz format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-excitement",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Memory usage\n",
    "For merging two sumstat with ~85000 rows and of size of ~5MB, 1 GB of memory is needed \n",
    "\n",
    "For merging two sumstat with ~2000000 rows and of size of ~1 GB, at least 50 GB of memory is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-injury",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## MWE Example command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-technique",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Target generation\n",
    "\n",
    "```\n",
    "sos run  pipeline/summary_stats_standardizer.ipynb   TARGET_generation  \\\n",
    "      --sumstat-list output/data_intergration/TensorQTL/qced_sumstat_list.txt    \\\n",
    "      --yml-list output/data_intergration/TensorQTL/yml_list.txt    \\\n",
    "      --fasta reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "      --cwd output/data_intergration/TensorQTL  -J 2 -c csg.yml -q csg --mem 50G --walltime 48h &\n",
    "```\n",
    "\n",
    "### sumstat_standardization\n",
    "\n",
    "```\n",
    "sos run  pipeline/summary_stats_standardizer.ipynb   sumstat_standardization  \\\n",
    "      --sumstat-list output/data_intergration/TensorQTL/qced_sumstat_list.txt    \\\n",
    "      --yml-list output/data_intergration/TensorQTL/yml_list.txt    \\\n",
    "      --TARGET_list output/data_intergration/TensorQTL/TARGET.ref.list \\\n",
    "      --cwd output/data_intergration/TensorQTL  -J 2 -c csg.yml -q csg --mem 50G --walltime 48h &\n",
    "```\n",
    "\n",
    "output/data_intergration/TensorQTL/MWE.3.yml.TARGET.ref.list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93586b-d872-40a4-895c-51aa1f766815",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run  pipeline/summary_stats_standardizer.ipynb   sumstat_to_vcf  \\\n",
    "      --sumstat-list  /mnt/vast/hpc/csg/ROSMAP_methy_QTL/data_intergration/TensorQTL/qced_sumstat_list.txt   \\\n",
    "      --cwd /mnt/vast/hpc/csg/ROSMAP_methy_QTL/data_intergration/TensorQTL/  -J 23 -c csg.yml -q csg2 --mem 50G --walltime 48h &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66275809-ad6c-41e3-a6dc-696a8810261d",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-precipitation",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import pandas as pd \n",
    "# Work directory where output will be saved to\n",
    "parameter: cwd = path(\"output\")\n",
    "\n",
    "#if add --keep-ambiguous parameter, keep ambiguous alleles which can not be decided from flip or reverse, such as A/T or C/G. Otherwise, remove them.\n",
    "parameter: keep_ambiguous = False\n",
    "# if add --intersect parameter, output intersect SNPs in all input files.\n",
    "parameter: intersect = False\n",
    "# Containers that contains the necessary packages\n",
    "parameter: container = \"\"\n",
    "parameter: numThreads = 1\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Walltime \n",
    "parameter: walltime = '5h'\n",
    "parameter: mem = '3G'\n",
    "# The directory of the output sumstat\n",
    "parameter: sumstat_list = path\n",
    "sumstat_path = pd.read_csv(sumstat_list,sep = \"\\t\").drop(columns=\"#chr\").values.tolist()\n",
    "name = pd.read_csv(sumstat_list,sep = \"\\t\").drop(columns=\"#chr\").columns.values.tolist()\n",
    "## Whether to rename the Chr name.\n",
    "parameter: remame = False\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "import time\n",
    "pd.DataFrame({\"A\" : list(range(1,23)) + [\"X\",\"Y\",\"MT\"],\"X\" : [ f'chr{x}' for x in  list(range(1,23)) + [\"X\",\"Y\",\"MT\"]]}).to_csv(f'{cwd}/chr_name',\"\\t\",header = None, index = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f2eb0-f132-439a-ae2d-cdacead7f076",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "related-northwest",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow codes\n",
    "The first session is to generate a stand alone target file that can be changed into vcf and then standardized based on GTF. It include three step: Take the union of all snps without allele fliping, create a pseudo-vcf file, use bcftools to standardized the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43ecbd-d5d0-4e44-b76e-af5ff0f3e1cb",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[TARGET_generation_1]\n",
    "## path to a list of yml file , with columns #chr and dir\n",
    "parameter: yml_list = path\n",
    "import pandas as pd\n",
    "yml_path = pd.read_csv(yml_list,sep = \"\\t\").values.tolist()\n",
    "chr_inv = [x[0] for x in yml_path]\n",
    "file_inv = [x[1] for x in yml_path]\n",
    "input: file_inv , group_by = 1, group_with = \"chr_inv\"\n",
    "output: f'{cwd}/{_input:bn}.{_chr_inv}.all_snp.vcf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container = container\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from cugg.sumstat import read_sumstat\n",
    "    from cugg.sumstat import ss_2_vcf\n",
    "    from cugg.utils import *\n",
    "    yml = load_yaml(\"${_input}\")\n",
    "    input_dict = parse_input(yml['INPUT'])\n",
    "    ## Remap the YML field name\n",
    "    new_key = [\"molecular_trait_id\",\"chromosome\", \"position\", \"ref\" ,\"alt\",\"variant\"]\n",
    "    old_key = [\"GENE\" ,  \"CHR\" ,  \"POS\" ,   \"A0\" ,   \"A1\",\"SNP\"]\n",
    "\n",
    "    for x in input_dict.values():\n",
    "        for i,j in zip(old_key,new_key):\n",
    "            x[i] = x.pop(j)\n",
    "            x[\"ID\"] = x[\"ID\"].replace(j,i)\n",
    "\n",
    "    def ss_2_vcf(ss_df,name = \"name\"):\n",
    "        ## Geno field\n",
    "        df = pd.DataFrame()\n",
    "        if \"SNP\" not in ss_df.columns:\n",
    "            ss_df['SNP'] = 'chr'+ss_df.CHR.astype(str).str.strip(\"chr\") + ':' + ss_df.POS.astype(str) + '_' + ss_df.A0.astype(str) + '_' + ss_df.A1.astype(str)\n",
    "        df[['#CHROM', 'POS', 'ID', 'REF', 'ALT']] = ss_df[['CHR', 'POS', 'SNP', 'A0', 'A1']]\n",
    "        ## Info field(Empty)\n",
    "        df['QUAL'] = \".\"\n",
    "        df['FILTER'] = \"PASS\"\n",
    "        df['INFO'] = \".\"\n",
    "        fix_header = [\"SNP\",\"A1\",\"A0\",\"POS\",\"CHR\",\"STAT\",\"SE\",\"P\"]\n",
    "        header_list = []\n",
    "        if \"GENE\" in ss_df.columns:\n",
    "            df['ID'] = ss_df['GENE'] + \":\" + ss_df['SNP']\n",
    "            df['INFO'] = \"GENE=\" + ss_df[\"GENE\"]\n",
    "            fix_header = [\"GENE\",\"SNP\",\"A1\",\"A0\",\"POS\",\"CHR\",\"STAT\",\"SE\",\"P\"]\n",
    "            header_list = ['##INFO=<ID=GENE,Number=1,Type=String,Description=\"The name of genes\">']\n",
    "        ### Fix headers\n",
    "        import time\n",
    "        header = '##fileformat=VCFv4.2\\n' + \\\n",
    "        '##FILTER=<ID=PASS,Description=\"All filters passed\">\\n' + \\\n",
    "        f'##fileDate={time.strftime(\"%Y%m%d\",time.localtime())}\\n'+ \\\n",
    "        '##FORMAT=<ID=STAT,Number=1,Type=Float,Description=\"Effect size estimate relative to the alternative allele\">\\n' + \\\n",
    "        '##FORMAT=<ID=SE,Number=1,Type=Float,Description=\"Standard error of effect size estimate\">\\n' + \\\n",
    "        '##FORMAT=<ID=P,Number=1,Type=Float,Description=\"The Pvalue corresponding to ES\">' \n",
    "        ### Customized Field headers\n",
    "        for x in ss_df.columns:\n",
    "            if x not in fix_header:\n",
    "                Prefix = f'##FORMAT=<ID={x},Number=1,Type='\n",
    "                Type = str(type(ss_df[x][0])).replace(\"<class \\'\",\"\").replace(\"'>\",\"\").replace(\"numpy.\",\"\").replace(\"64\",\"\").capitalize().replace(\"Int\",\"Integer\")\n",
    "                Surfix = f',Description=\"Customized Field {x}\">'\n",
    "                header_list.append(Prefix+Type+Surfix)\n",
    "        ## format and sample field\n",
    "        df['FORMAT'] = \":\".join([\"STAT\",\"SE\",\"P\"]  + ss_df.drop(fix_header,axis = 1).columns.values.tolist())\n",
    "        df[f'{name}'] = ss_df.drop( [\"SNP\",\"A1\",\"A0\",\"POS\",\"CHR\"],axis = 1).astype(str).apply(\":\".join,axis = 1)\n",
    "        ## Rearrangment\n",
    "        df = df[['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO','FORMAT',f'{name}']]\n",
    "        df = df.sort_values(['#CHROM', 'POS'])\n",
    "        # Add headers\n",
    "        header = header + \"\\n\".join(header_list) + \"\\n\"\n",
    "        return df,header\n",
    "\n",
    "    ## Verify data uniqueness\n",
    "    lst_sumstats_file = [ os.path.basename(i) for i in input_dict.keys()]\n",
    "    if len(set(lst_sumstats_file))<len(lst_sumstats_file):\n",
    "        raise Exception(\"There are duplicated names in {}\".format(lst_sumstats_file))\n",
    "    #read all sumstats\n",
    "    print(input_dict)\n",
    "    lst_sumstats = {os.path.basename(i):read_sumstat(i,j,) for i,j in input_dict.items()}\n",
    "    ## Retaining only chrom/pos/ref/alt, and dropping the duplicates (drop twice to reduce mem usage)\n",
    "    union_snp = pd.concat([x[[\"CHR\" ,  \"POS\" ,   \"A0\" ,   \"A1\",\"SNP\"]].drop_duplicates() for x in lst_sumstats.values() ]).drop_duplicates() \n",
    "    ## Create fake header\n",
    "    union_snp[[\"STAT\",\"SE\",\"P\"]] = 1\n",
    "    sumstats,header = ss_2_vcf(union_snp,\"PseudoVCF\")\n",
    "    with open(${_output:r}, 'w') as f:\n",
    "        f.write(header)\n",
    "    sumstats.to_csv(${_output:r}, sep = \"\\t\", header = True, index = False,mode = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88c503-14e2-4839-931c-a4b0f9c5297b",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[TARGET_generation_2]\n",
    "## The reference fasta\n",
    "parameter: fasta = path\n",
    "output: f'{_input:nn}.TARGET.ref'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = '${ }', stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container = container\n",
    "    echo -e \"CHR\\tPOS\\tA0\\tA1\" > ${_output}\n",
    "    bgzip -f ${_input}\n",
    "    tabix -p vcf -f  ${_input}.gz\n",
    "    ## our fasta required chr* as chromosome name format\n",
    "    bcftools annotate --rename-chrs ${cwd}/chr_name ${_input}.gz -Oz | \\\n",
    "    bcftools norm  -N --check-ref ws -f ${fasta} | bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT\\n' >> ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d89cd-28d4-4789-bfd6-8ef4fb47acef",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[TARGET_generation_3]\n",
    "input: group_by = \"all\"\n",
    "output: f'{cwd}/TARGET.ref.list'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', stderr = f'{_output}.stderr', stdout = f'{_output}.stdout',container = container\n",
    "    import pandas as pd\n",
    "    target_path = [${_input:r,}]\n",
    "    chrom = [x.split(\".\")[-3].replace(\"chr\",\"\") for x in target_path ]\n",
    "    pd.DataFrame({\"#chr\": chrom , \"TARGET\" : target_path }).to_csv(\"${_output}\",\"\\t\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "frequent-ballot",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sumstat_standardization]\n",
    "## path to a list of yml file , with columns #chr and dir\n",
    "parameter: yml_list = path\n",
    "import pandas as pd\n",
    "yml_path = pd.read_csv(yml_list,sep = \"\\t\").values.tolist()\n",
    "depends: Py_Module('cugg')\n",
    "parameter: TARGET_list = path\n",
    "TARGET_path = pd.read_csv(TARGET_list,sep = \"\\t\")\n",
    "yml_path = pd.read_csv(yml_list,sep = \"\\t\").merge(TARGET_path, on = \"#chr\").values.tolist()\n",
    "file_inv = [x[1] for x in yml_path]\n",
    "TARGET_inv = [x[2] for x in yml_path]\n",
    "input: file_inv , group_by = 1, group_with = \"TARGET_inv\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', stderr = f'{_input}.stderr', stdout = f'{_input}.stdout',container = container\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from cugg.sumstat import read_sumstat\n",
    "    from cugg.utils import *\n",
    "    \n",
    "    yml = \"${_input}\"\n",
    "    keep_ambiguous = ${keep_ambiguous}\n",
    "    intersect = ${intersect}\n",
    "    print(yml, keep_ambiguous,intersect)\n",
    "    #parse yaml\n",
    "    yml = load_yaml(yml)\n",
    "    input_dict = parse_input(yml['INPUT'])\n",
    "    ## Remap the YML field name\n",
    "    new_key = [\"molecular_trait_id\",\"chromosome\", \"position\", \"ref\", \"alt\",\"beta\",\"se\",\"pvalue\",\"variant\"]\n",
    "    old_key = [\"GENE\" ,  \"CHR\" ,  \"POS\" ,   \"A0\" ,   \"A1\" ,\"STAT\",\"SE\",\"P\",\"SNP\"]\n",
    "    name_map = {old_key[i]: new_key[i] for i in range(len(old_key))}\n",
    "    name_map_rev = {new_key[i]: old_key[i] for i in range(len(old_key))}\n",
    "    for x in input_dict.values():\n",
    "        for k, v in list(x.items()):\n",
    "            x[name_map_rev.get(k, k)] = x.pop(k)\n",
    "        for i,j in zip(old_key,new_key):\n",
    "            x[\"ID\"] = x[\"ID\"].replace(j,i)\n",
    "    target_dict = \"${_TARGET_inv}\"\n",
    "    output_path = yml['OUTPUT'][0]\n",
    "    lst_sumstats_file = [ os.path.basename(i) for i in input_dict.keys()]\n",
    "    print('Total number of sumstats: ',len(lst_sumstats_file))\n",
    "    if len(set(lst_sumstats_file))<len(lst_sumstats_file):\n",
    "        raise Exception(\"There are duplicated names in {}\".format(lst_sumstats_file))\n",
    "    #read all sumstats\n",
    "    print(input_dict)\n",
    "    lst_sumstats = {os.path.basename(i):read_sumstat(i,j,) for i,j in input_dict.items()}\n",
    "    nqs = []\n",
    "    #Readin the reference target file\n",
    "    subject = check_indels(read_sumstat(target_dict,None,True)[[\"CHR\",\"POS\",\"SNP\",\"A0\",\"A1\"]])\n",
    "    if \"*\" in subject.A0.values: \n",
    "        raise ValueError(f'illegal character \"*\" is in the REF column of the TARGET, please check the TARGET file with a reference')\n",
    "    for query in lst_sumstats.values():\n",
    "        #check duplicated indels and remove them.\n",
    "        query = check_indels(query)\n",
    "        # Set the snp column to be the second column to satisify the requirement of compare_snps() function\n",
    "        column =  query.pop(\"SNP\")\n",
    "        query.insert(2,\"SNP\", column )\n",
    "        #under the same chr:pos or gene:chr:pos. match A0 and A1 by exact, flip, reverse, or flip+reverse.\n",
    "        #if duplicated chr_pos or gene_chr_pos exist, run a recursive match for each pair of them between query and subject.\n",
    "        # If GENE info is in query but not subject, added it.\n",
    "        if \"GENE\" in query.columns and \"GENE\" not  in subject.columns:\n",
    "            subject = subject.merge(query[[\"GENE\",\"CHR\",\"POS\"]]).drop_duplicates().sort_values(\"GENE\")\n",
    "            ## It is crucial that the index was built via this function, where the order of A0/A1 was removed. Otherwise will cause error is issue #306\n",
    "            subject.index = namebyordA0_A1(subject[[\"GENE\",\"CHR\",\"POS\",\"A0\",\"A1\"]],cols=[\"GENE\",\"CHR\",\"POS\",\"A0\",\"A1\"])\n",
    "\n",
    "        nq,_ = snps_match(query,subject,keep_ambiguous)\n",
    "        nq = nq.loc[:,~nq.columns.duplicated()] # Remove duplicated columns due to order of columns difference in subject and query\n",
    "        nqs.append(nq)\n",
    "    if intersect:\n",
    "        #get common snps\n",
    "        common_snps = set.intersection(*[set(nq.SNP) for nq in nqs])\n",
    "        print('Total number of common SNPs: ',len(common_snps))\n",
    "        #write out new sumstats\n",
    "        for output_sumstats,nq in zip(lst_sumstats_file,nqs):\n",
    "            sumstats = nq[nq.SNP.isin(common_snps)]\n",
    "            sumstats[\"variants\"] = sumstats.CHR.astype(str) + \"_\" + sumstats.POS.astype(str) + \"_\" + sumstats.A0 + \"_\" + sumstats.A1\n",
    "            sumstats = sumstats.rename(columns = name_map )\n",
    "            sumstats.to_csv(os.path.join(output_path, output_sumstats), sep = \"\\t\", header = True, index = False)\n",
    "    else:\n",
    "        for output_sumstats,nq in zip(lst_sumstats_file,nqs):\n",
    "            nq[\"variants\"] = nq.CHR.astype(str) + \"_\" + nq.POS.astype(str) + \"_\" + nq.A0 + \"_\" + nq.A1\n",
    "            nq = nq.rename(columns = name_map )\n",
    "            #output match SNPs with target SNPs.\n",
    "            nq.to_csv(os.path.join(output_path, output_sumstats), sep = \"\\t\", header = True, index = False)\n",
    "    print('All are done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expanded-civilization",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-teach",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sumstat_to_vcf_1 ]\n",
    "input:  for_each = \"sumstat_path\"\n",
    "output: [f'{path(x):an}.vcf' for x in _sumstat_path]\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', stderr = f'{cwd:a}/{path(_sumstat_path[0]):bn}.stderr', stdout = f'{cwd:a}/output.stdout'\n",
    "    from cugg.sumstat import ss_2_vcf\n",
    "    import pandas as pd\n",
    "    from sos.targets import path\n",
    "    sumstat_path_list = ${_sumstat_path}\n",
    "    name = ${name}\n",
    "    ## Remap the YML field name\n",
    "    new_key = [\"molecular_trait_id\",\"chromosome\", \"position\", \"ref\", \"alt\",\"beta\",\"se\",\"pvalue\",\"variant\"]\n",
    "    old_key = [\"GENE\" ,  \"CHR\" ,  \"POS\" ,   \"A0\" ,   \"A1\" ,\"STAT\",\"SE\",\"P\",\"SNP\"]\n",
    "    name_map = {old_key[i]: new_key[i] for i in range(len(old_key))}\n",
    "    name_map_rev = {new_key[i]: old_key[i] for i in range(len(old_key))}\n",
    "    for x,y in zip(sumstat_path_list,name):\n",
    "        sumstats = pd.read_csv(x,\"\\t\").rename(columns = name_map_rev )\n",
    "        sumstats,header = ss_2_vcf(sumstats,y)\n",
    "        with open(f'{path(x):an}.vcf', 'w') as f:\n",
    "            f.write(header)\n",
    "        sumstats.to_csv(f'{path(x):an}.vcf', sep = \"\\t\", header = True, index = False,mode = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-reception",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sumstat_to_vcf_2]\n",
    "output: f'{cwd}/{_input[0]:bn}.merged.vcf.gz'.replace(name[0],\"_\".join(name))\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = '${ }', stderr = f'{cwd:a}/{_output:bn}.stderr', stdout = f'{cwd:a}/{_output:bn}.stdout',container = container\n",
    "    for i in ${_input:r}; do\n",
    "    bgzip -k -f $i \n",
    "    tabix -p vcf -f  $i.gz; done\n",
    "    bcftools merge ${\" \".join([f'{str(x)}.gz' for x in _input])} --force-samples -m id  -Oz -o ${_output:a}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
