{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "friendly-collector",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Fine-mapping with PolyFun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-electric",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-midwest",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "The purpose of this notebook ipmlements commands for [a functionally-informed fine-mapping workflow using the PolyFun method](https://github.com/omerwe/polyfun/wiki)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-friday",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Methods Overview \n",
    "\n",
    "`PolyFun` offers the following features:\n",
    "\n",
    "1. Using and/or creating Functional Annotations\n",
    "2. Estimating Functional Enrichment Using `S-LDSC`\n",
    "3. Using and/or computating Prior Causal Probabilities from 1\n",
    "4. Functionally Informed Fine Mapping with Finemapper\n",
    "5. Polygenic Localization with `PolyLoc`\n",
    "\n",
    "**Notice: this workflow does not implements 5 the `PolyLoc`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-planning",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-grade",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1) GWAS summary statistics including the following variables: \n",
    "\n",
    "    - variant_id - variant ID \n",
    "    - P - p-value \n",
    "    - CHR - chromosome number \n",
    "    - BP - base pair position\n",
    "    - A1 - The effect allele (i.e., the sign of the effect size is with respect to A1)\n",
    "    - A2 - the second allele \n",
    "    - MAF - minor allele frequency \n",
    "    - BETA - effect size \n",
    "    - SE - effect size standard error\n",
    "\n",
    "\n",
    "2) Functional annotation files including the following columns: \n",
    "\n",
    "    - CHR - chromosome number\n",
    "    - BP base pair position (in hg19 coordinates)\n",
    "    - SNP - dbSNP reference number \n",
    "    - A1 - The effect allele \n",
    "    - A2 - the second allele\n",
    "    - Arbitrary additional columns representing annotations\n",
    "\n",
    "\n",
    "3) A `.l2.M` white-space delimited file containing a single line with the sums of the columns of each annotation\n",
    "\n",
    "4) LD-score files \n",
    "\n",
    "    - Strongly recommended that LD-score files include A1,A2 columns\n",
    "\n",
    "\n",
    "5) LD information, taken from one of three possible data sources:\n",
    "\n",
    "    - plink files with genotypes from a reference panel\n",
    "    - bgen file with genotypes from a reference panel\n",
    "    - pre-computed LD matrix\n",
    "\n",
    "    Optional if (4) is obtained and no plans to compute prior causal probabilities non-parametrically \n",
    "\n",
    "6) Ld-score weights files.\n",
    "\n",
    "    - Strongly recommended that weight files include A1,A2 columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-tablet",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Output\n",
    "\n",
    "A `.gz` file containing input summary statistics columns and additionally the following columns:\n",
    "\n",
    "- PIP - posterior causal probability\n",
    "- BETA_MEAN - posterior mean of causal effect size (in standardized genotype scale)\n",
    "- BETA_SD - posterior standard deviation of causal effect size (in standardized genotype scale)\n",
    "- CREDIBLE_SET - the index of the first (typically smallest) credible set that the SNP belongs to (0 means none).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-female",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-matrix",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Step 1 and 2 are optional if using pre-computed prior causal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-simple",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 1: Obtain functional annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-sellers",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For each chromosome, the following files need to be obtained: \n",
    "\n",
    "1) A `.gz` or `.parquet` annotations file containing the following columns:\n",
    "\n",
    "- CHR - chromosome number\n",
    "- BP base pair position\n",
    "- SNP - dbSNP reference number \n",
    "- A1 - The effect allele \n",
    "- A2 - the second allele\n",
    "- Arbitrary additional columns representing annotations \n",
    "\n",
    "2) A `.l2.M` white-space delimited file containing a single line with the sums of the columns of each annotation\n",
    "\n",
    "3) (Optional) A `l2.M_5_50` file that is the `.l2.M` file but only containing common SNPS (MAF between 5% and 50%) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-resolution",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The above files can be obtained either by using existing function annotation files, or by creating your own through other software such as `TORUS`.\n",
    "\n",
    "Existing function annotation files example: functional annotations for ~19 million UK Biobank imputed SNPs with MAF>0.1%, based on the baseline-LF 2.2.UKB annotations.\n",
    "\n",
    "Download (30G): https://data.broadinstitute.org/alkesgroup/LDSCORE/baselineLF_v2.2.UKB.polyfun.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-terrorism",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 2: Compute LD-scores for annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-papua",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Precomputed LD-score files can be used. LD-score files can also be generated through the methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-geology",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Compute with reference panel of sequenced individuals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-worth",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Reference panel should have at least 3000 sequenced individuals from target population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-prefix",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: container = \"/mnt/mfs/statgen/containers/xqtl_pipeline_sif/polyfun.sif\"\n",
    "parameter: wd = path(\"./\")\n",
    "parameter: exe_dir = \"/usr/local/bin/\"\n",
    "parameter: name = \"demo\"\n",
    "parameter: genoFile = path(\"./\")\n",
    "parameter: annot_file = path(\"./\")\n",
    "parameter: sumstats = path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-slovenia",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score]\n",
    "input: annot_file, genoFile\n",
    "output: f'{wd:a}/{name}.ref.ldscore.parquet'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '30G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/compute_ldscores.py \\\n",
    "        --bfile $[_input[1]:n] \\\n",
    "        --annot $[_input[0]] \\\n",
    "        --out $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-birth",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Compute with pre-computed UK Biobank LD matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-instrument",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Matrices download: https://data.broadinstitute.org/alkesgroup/UKBB_LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-samoa",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score_ukb]\n",
    "input: annot_file\n",
    "output: f'{wd:a}/{name}.ukb.ldscore.parquet'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '30G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/compute_ldscores_from_ld.py \\\n",
    "        --annot $[_input[0]] \\\n",
    "        --ukb \\\n",
    "        --out $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-understanding",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 3: Compute with own pre-computed LD matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-residence",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Own pre-computed LD matrices should be in `.bcor` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-composer",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score_own]\n",
    "parameter: sample_size = int\n",
    "parameter: bcor_files = paths\n",
    "input: annot_file,bcor_files\n",
    "output: f'{wd:a}/{name}.original.ldscore.parquet'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '30G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/compute_ldscores_from_ld.py $[_input[1]] \\\n",
    "        --annot $[_input[0]] \\\n",
    "        --out $[_output] \\\n",
    "        --n $[sample_size] \\\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-bernard",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 3: Compute Prior Causal Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-brief",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Use precomputed prior causal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-carpet",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Use precomputed prior causal probabilities of 19 million imputed UK Biobank SNPs with MAF>0.1%, based on a meta-analysis of 15 UK Biobank traits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-light",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[prior_causal_prob]\n",
    "input: sumstats\n",
    "output: f'{wd:a}/{name}.pcp.gz'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '30G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    python $[exe_dir]/extract_snpvar.py \\\n",
    "        --sumstats $[_input] \\\n",
    "        --out $[_output] \\\n",
    "        --allow-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-commander",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Compute via L2-regularized extension of S-LDSC (preferred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-survivor",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Compute via an L2-regularized extension of stratified LD-score regression (S-LDSC). Use the annotation and LD-score files produced in Step1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-deviation",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1) Create a munged summary statistics file in a PolyFun-friendly parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-joshua",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[munged_sumstats]\n",
    "parameter: sample_size = 472868 \n",
    "parameter: min_info = 0.6\n",
    "parameter: min_maf = 0.01\n",
    "input: sumstats\n",
    "output: f'{wd:a}/{name}.sumstats_munged.parquet'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '127G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/munge_polyfun_sumstats.py \\\n",
    "      --sumstats $[_input] \\\n",
    "      --n $[sample_size] \\\n",
    "      --out $[_output] \\\n",
    "      --min-info $[min_info] \\\n",
    "      --min-maf $[min_maf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-parallel",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "2) Run PolyFun with L2-regularized S-LDSC\n",
    "- Require at least 45 GB of mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-structure",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[L2_SLDSC]\n",
    "# a ld score file with surfix l2.ldscore.parquet\n",
    "parameter: ref_ld = path\n",
    "# another ld score file with surfix l2.ldscore.parquet, different from ref_ld\n",
    "parameter: ref_wgt = ref_ld\n",
    "parameter: partitions = \"\"\n",
    "input: ref_ld, ref_wgt,output_from(\"munged_sumstats\")\n",
    "# parameter: sumstat = _input[2]\n",
    "output: f'{wd:a}/{name}.ldsldsc.parquent'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '127G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/polyfun.py \\\n",
    "        --compute-h2-L2 \\\n",
    "        --output-prefix $[_output] \\\n",
    "        --sumstats $[_input[2]] \\\n",
    "        --ref-ld-chr $[_input[0]:nnnn].\\\n",
    "        --w-ld-chr $[_input[1]:nnnn]. \\\n",
    "        --allow-missing $[\"\" if partitions else \"--no-partitions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-premiere",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 3: Compute Non-parametrically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-chase",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1) Create a munged summary statistics file in a PolyFun-friendly parquet format.\n",
    "Duplicated cells are commented out, the input of [ld_snpbin] is the output from [L2_regu_SLDSC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-concord",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[munged_sumstats2]\n",
    "#parameter: sumstats = AD_sumstats_Jansenetal_2019sept.txt.gz\n",
    "#parameter: sample_size = int\n",
    "#parameter: container = none\n",
    "#bash: container = container \n",
    "#    mkdir -p SLDSC_output\n",
    "#    python munge_polyfun_sumstats.py \\\n",
    "#      --sumstats sumstats \\\n",
    "#      --n sample_size \\\n",
    "#      --out /SLDSC_output/sumstats_munged.parquet \\\n",
    "#      --min-info 0 \\\n",
    "#      --min-maf 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-richardson",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "2) Run PolyFun with L2-regularized S-LDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-cosmetic",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# [L2_regu_SLDSC2]\n",
    "# \n",
    "# parameter: container = none\n",
    "# paramter: ref_ld = example_data/annotations.\n",
    "# parameter: ref_wgt = example_data/weights.\n",
    "# bash: container=container\n",
    "#     python polyfun.py \\\n",
    "#     --compute-h2-L2 \\\n",
    "#     --output-prefix output/testrun \\\n",
    "#     --sumstats example_data/sumstats.parquet \\\n",
    "#     --ref-ld-chr ref_ld \\\n",
    "#     --w-ld-chr ref_wgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-sociology",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "3) Compute LD-scores for each SNP bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-screw",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_snpbin]\n",
    "depends: sos_step(\"L2_regu_SLDSC\")\n",
    "parameter: chrom = int\n",
    "input: annot_file, genoFile\n",
    "output: f'{wd:a}/{name}.snpbin.ldscore.parquet'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '30G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "     python $[exe_dir]/polyfun.py \\\n",
    "        --compute-ldscores \\\n",
    "        --bfile-chr $[_input[1]:n] \\\n",
    "        --output-prefix $[_output] \\\n",
    "        --chr $[chrom]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-norman",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "4) Re-estimate per-SNP heritabilities via S-LDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-airfare",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#[re_SLDSC]\n",
    "#bash:\n",
    "#    python polyfun.py \\\n",
    "#    --compute-h2-bins \\\n",
    "#    --output-prefix output/testrun \\\n",
    "#    --sumstats example_data/sumstats.parquet \\\n",
    "#    --w-ld-chr example_data/weights.\n",
    "\n",
    "[L2_SLDSC_bins]\n",
    "paramter: ref_ld = path\n",
    "parameter: ref_wgt = ref_ld\n",
    "parameter: partitions = \"\"\n",
    "input: ref_ld, ref_wgt,output_from(\"munged_sumstats\")\n",
    "output: f'{wd:a}/{name}.txt.gz'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '30G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/polyfun.py \\\n",
    "        --compute-h2-bins \\\n",
    "        --output-prefix $[_output] \\\n",
    "        --sumstats $[_input[2]] \\\n",
    "        --ref-ld-chr $[_input[0]]\\\n",
    "        --w-ld-chr $[_input[1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-credits",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 4: Functionally informed fine mapping with finemapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-photography",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Input summary statistics file must have `SNPVAR` column (per-SNP heritability) to perform functionally-informed fine-mapping. To fine-map without annotations, use additional parameter `--non-funct`. The summary statistical file then will not require the `SNPVAR` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-miami",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fine_mapping]\n",
    "parameter: sample_size = 383290\n",
    "parameter: chrom = 1\n",
    "parameter: start = 46000001\n",
    "parameter: end = 49000001\n",
    "#parameter: output_path = \"output/finemap.1.46000001.49000001.gz\"\n",
    "parameter: max_num_causal = 5\n",
    "input: genoFile,sumstats\n",
    "output: f'{wd:a}/output/finemap.{chrom}.{start}.{end}.gz'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '30G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/finemapper.py \\\n",
    "        --geno $[_input[0]:n] \\\n",
    "        --sumstats $[_input[1]]  \\\n",
    "        --n $[sample_size] \\\n",
    "        --chr $[chrom] \\\n",
    "        --start $[start] \\\n",
    "        --end $[end] \\\n",
    "        --method susie \\\n",
    "        --max-num-causal $[max_num_causal] \\\n",
    "        --cache-dir $[_output:d]/cache \\\n",
    "        --out $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-difference",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Minimal Working Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-society",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Example 1: Functionally-informed fine-mapping using summary statistics file with precomputed prior causal probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-compromise",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "nohup sos run ~/GIT/xqtl-pipeline/pipeline/integrative_analysis/SuSiE_Ann/polyfun.ipynb prior_causal_prob \\\n",
    "    --sumstats /home/at3535/polyfun/AD_sumstats_Jansenetal_2019sept.txt.gz  \\\n",
    "    -J 200 -q csg \\\n",
    "    -c /home/hs3163/GIT/ADSPFG-xQTL/code/csg.yml &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tight-fantasy",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "nohup sos run ~/GIT/xqtl-pipeline/pipeline/integrative_analysis/SuSiE_Ann/polyfun.ipynb fine_mapping \\\n",
    "    --sumstats /home/at3535/polyfun/AD_sumstats_Jansenetal_2019sept.txt.gz  \\\n",
    "    --genoFile /mnt/mfs/statgen/ROSMAP_xqtl/dataset/snvCombinedPlink/chr1.bed \\\n",
    "    -J 200 -q csg \\\n",
    "    -c /home/hs3163/GIT/ADSPFG-xQTL/code/csg.yml &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-concert",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Example 2: Functionally-informed fine-mapping using summary statistics file generated from pre-obtained annotation and LD-score files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-yeast",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "nohup sos run ~/GIT/xqtl-pipeline/pipeline/integrative_analysis/SuSiE_Ann/polyfun.ipynb munged_sumstats  \\\n",
    "--sumstats /home/at3535/polyfun/GCST90012877_buildGRCh37_colrenamed.txt.gz \\\n",
    "-J 200 -q csg -c /home/hs3163/GIT/ADSPFG-xQTL/code/csg.yml &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-dodge",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "nohup sos run ~/GIT/xqtl-pipeline/pipeline/integrative_analysis/SuSiE_Ann/polyfun.ipynb L2_SLDSC  \\\n",
    "--sumstats demo.sumstats_munged.parquet \\\n",
    "--ref_ld /mnt/mfs/statgen/tl3030/baselineLF2.2.UKB/baselineLF2.2.UKB.1.l2.ldscore.parquet \\\n",
    "-J 200 -q csg -c /home/hs3163/GIT/ADSPFG-xQTL/code/csg.yml &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-indonesian",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "nohup sos run ~/GIT/xqtl-pipeline/pipeline/integrative_analysis/SuSiE_Ann/polyfun.ipynb fine_mapping \\\n",
    "    --sumstats demo.ldsldsc.parquent  \\\n",
    "    --genoFile /mnt/mfs/statgen/ROSMAP_xqtl/dataset/snvCombinedPlink/chr1.bed \\\n",
    "    -J 200 -q csg -c /home/hs3163/GIT/ADSPFG-xQTL/code/csg.yml &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-diamond",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-programming",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "processed-insured",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHR\tSNP\tBP\tA1\tA2\tSNPVAR\tZ\tN\tP\tPIP\tBETA_MEAN\tBETA_SD\tDISTANCE_FROM_CENTER\tCREDIBLE_SET\n",
      "1\trs2088102\t46032974\tT\tC\t1.70060e-06\t1.25500e+01\t383290\t3.97510e-36\t1.00000e+00\t-2.03917e-02\t1.61901e-03\t1456799\t1\n",
      "1\trs7528714\t47966058\tG\tA\t1.18040e-06\t5.14320e+00\t383290\t2.70098e-07\t9.97870e-01\t7.42146e-03\t1.62305e-03\t476285\t2\n",
      "1\trs7528075\t47870271\tG\tA\t1.18040e-06\t4.40160e+00\t383290\t1.07456e-05\t9.76545e-01\t-5.98945e-03\t1.81667e-03\t380498\t3\n",
      "1\trs212968\t48734666\tG\tA\t1.70060e-06\t-3.01130e+00\t383290\t2.60132e-03\t3.75823e-01\t-1.56305e-03\t2.23942e-03\t1244893\t0\n",
      "1\trs2622911\t47837404\tC\tA\t1.70060e-06\t3.12520e+00\t383290\t1.77684e-03\t3.71804e-01\t1.54312e-03\t2.22812e-03\t347631\t0\n",
      "1\trs4511165\t48293181\tG\tA\t1.70060e-06\t-1.18940e+00\t383290\t2.34282e-01\t5.75970e-02\t1.60630e-04\t7.52226e-04\t803408\t0\n",
      "1\trs3766196\t47284526\tC\tA\t6.93040e-06\t-5.92360e-02\t383290\t9.52764e-01\t4.89776e-02\t-5.06776e-07\t3.48039e-04\t205247\t0\n",
      "1\trs12567716\t48197570\tT\tC\t1.18040e-06\t2.14810e+00\t383290\t3.17058e-02\t4.45128e-02\t-1.28281e-04\t6.81457e-04\t707797\t0\n",
      "1\trs4927234\t48384796\tG\tA\t1.70060e-06\t-7.59600e-01\t383290\t4.47494e-01\t3.41123e-02\t7.74978e-05\t5.04755e-04\t895023\t0\n"
     ]
    }
   ],
   "source": [
    "bash:\n",
    "    gzcat output/finemap.1.46000001.49000001.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cellular-intention",
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Number of variants with PIP > 0.5: 3\\n', 'Number of variants with PIP > 0.95: 3\\n', 'Number of variants that have credible sets: 3\\n', 'Number of unique credible sets: 3\\n', 'Average number of variants per credible set: 1.0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('output/finemap.1.46000001.49000001.gz', sep=\"\\t\")\n",
    "\n",
    "data.head(5)\n",
    "    \n",
    "num_var_cs = np.count_nonzero(data['CREDIBLE_SET'])\n",
    "total_cs = len(data.CREDIBLE_SET.unique())- 1\n",
    "avg_var_cs = float(num_var_cs) / total_cs\n",
    "pip50 = sum(1 for i in data['PIP'] if i >0.5)\n",
    "pip95 = sum(1 for i in data['PIP'] if i >0.95)\n",
    "\n",
    "result = \"Number of variants with PIP > 0.5: \" + str(pip50) + \"\\n\" + \"Number of variants with PIP > 0.95: \" + str(pip95) + \"\\n\" \\\n",
    "    + \"Number of variants that have credible sets: \" + str(num_var_cs) + \"\\n\" \\\n",
    "    + \"Number of unique credible sets: \" + str(total_cs) + \"\\n\" \\\n",
    "    + \"Average number of variants per credible set: \" + str(avg_var_cs) \n",
    "\n",
    "\n",
    "with open('results.txt', 'a') as the_file:\n",
    "    the_file.write(result)\n",
    "\n",
    "with open('results.txt') as f:\n",
    "    contents = f.readlines()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-identifier",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "#### Example 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-language",
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "bash:\n",
    "    gzcat output/finemap.1.460000010.49000000.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adolescent-stereo",
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Number of variants with PIP > 0.5: 3\\n', 'Number of variants with PIP > 0.95: 3\\n', 'Number of variants that have credible sets: 3\\n', 'Number of unique credible sets: 3\\n', 'Average number of variants per credible set: 1.0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('output/finemap.1.46000000.49000000.gz', sep=\"\\t\")\n",
    "\n",
    "data.head(5)\n",
    "    \n",
    "num_var_cs = np.count_nonzero(data['CREDIBLE_SET'])\n",
    "total_cs = len(data.CREDIBLE_SET.unique())- 1\n",
    "avg_var_cs = float(num_var_cs) / total_cs\n",
    "pip50 = sum(1 for i in data['PIP'] if i >0.5)\n",
    "pip95 = sum(1 for i in data['PIP'] if i >0.95)\n",
    "\n",
    "result = \"Number of variants with PIP > 0.5: \" + str(pip50) + \"\\n\" + \"Number of variants with PIP > 0.95: \" + str(pip95) + \"\\n\" \\\n",
    "    + \"Number of variants that have credible sets: \" + str(num_var_cs) + \"\\n\" \\\n",
    "    + \"Number of unique credible sets: \" + str(total_cs) + \"\\n\" \\\n",
    "    + \"Average number of variants per credible set: \" + str(avg_var_cs) \n",
    "\n",
    "\n",
    "with open('results.txt', 'a') as the_file:\n",
    "    the_file.write(result)\n",
    "\n",
    "with open('results.txt') as f:\n",
    "    contents = f.readlines()\n",
    "    print(contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
