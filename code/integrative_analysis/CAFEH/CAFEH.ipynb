{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf81a4d-adb7-42d4-bb12-7143cf6ced94",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CAFEH\n",
    "1. generate LD for selected gene based on the per-gene genotype\n",
    "2. Extract and save sumstat s/beta/z from each RDS for selected gene\n",
    "3. Run CAFEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5385179-65a3-4a80-9ed5-2bedb0ed4945",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "nohup sos run /home/hs3163/GIT/xqtl-pipeline/code/integrative_analysis/CAFEH/CAFEH.ipynb CAFEH \\\n",
    "    --genoFile GRCh38_plink_files_list.txt  \\\n",
    "    --analysis-unit ALL_Ast_End_Exc_Inh_Mic_OPC_Oli.merged_rds.list \\\n",
    "    --region_list AD_genes.region_list -n  &\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b9de82-94a4-49b8-955a-7f635075bbc8",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import glob\n",
    "import pandas as pd\n",
    "# Input\n",
    "parameter: genoFile = path\n",
    "parameter: analysis_unit = path\n",
    "parameter: region_list = path\n",
    "parameter: cwd = path(\"output\")\n",
    "parameter: name = \"demo\"\n",
    "region_tbl = pd.read_csv(region_list,sep = \"\\t\")\n",
    "input_inv = pd.read_csv(genoFile,sep = \"\\t\",names = [\"gene_id\",\"geno_path\"],header = 0).merge(region_tbl,on = \"gene_id\").merge(pd.read_csv(analysis_unit,sep = \"\\t\",names = [\"gene_name\",\"rds_path\"],header = 0),on = \"gene_name\")\n",
    "gene_inv = input_inv.gene_id.tolist()\n",
    "geno_inv = input_inv.geno_path.tolist()\n",
    "ss_inv = input_inv.rds_path.tolist()\n",
    "## sampleSheetAfterQC.filtered_geno.txt\n",
    "parameter: sample_to_keep = \"sampleSheetAfterQC.filtered_geno.txt\"\n",
    "parameter: sample_size = 415\n",
    "## Containers that contains the necessary packages\n",
    "parameter: container = \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 20\n",
    "# Allow exclude some study\n",
    "parameter: drop_study = [\"-1\",\"0\"]\n",
    "# use this function to edit memory string for PLINK input\n",
    "from sos.utils import expand_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c41e1b-7d38-40c6-a414-4448d66b2d33",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10234383-2ddc-402b-9415-eea68c7048d7",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65219505-5992-40dc-a654-fe1907ba6f49",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f628bf1-506c-40ca-8367-7a51943892ad",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[LD]\n",
    "input: geno_inv, group_by = 1\n",
    "output: f'{_input:n}.ld',f'{_input:n}.named.ld.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container, volumes = [f'{_input:ad}:{_input:ad}']\n",
    "    plink \\\n",
    "          --bfile ${_input:n} \\\n",
    "          --keep ${sample_to_keep} \\\n",
    "          --out ${_output[0]:n} \\\n",
    "          --threads ${numThreads} \\\n",
    "          --memory ${int(expand_size(mem) * 0.9)/1e6} --r square\n",
    "python: expand= \"${ }\", stderr = f'{_output[1]}.stderr', stdout = f'{_output[1]}.stdout', container = container, volumes = [f'{_input:ad}:{_input:ad}']\n",
    "    import pandas as pd\n",
    "    ld  = pd.read_csv(\"${_output[0]}\",\"\\t\",header = None)\n",
    "    snp  = pd.read_csv(\"${_input:n}.bim\",\"\\t\",header = None).iloc[:,1].tolist()\n",
    "    ld.index = snp\n",
    "    ld.columns = snp\n",
    "    ld.to_csv(\"${_output[1]}\",\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c05751-1b5d-4f02-8275-b965d118354b",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[Sumstat]\n",
    "input: ss_inv, group_by = 1\n",
    "output: f'{_input:n}.bhat.tsv',f'{_input:n}.sbhat.tsv',f'{_input:n}.z.tsv',f'{_input:n}.n.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand= \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container, volumes = [f'{_input:ad}:{_input:ad}']\n",
    "    data = readRDS(\"${_input}\")\n",
    "    write.table((t(data$bhat)), \"${_output[0]}\", sep = \"\\t\", quote = FALSE)\n",
    "    write.table((t(data$sbhat)), \"${_output[1]}\", sep = \"\\t\", quote = FALSE)\n",
    "    write.table((t(data$Z)), \"${_output[2]}\", sep = \"\\t\", quote = FALSE)\n",
    "    n = t(data$Z)\n",
    "    n[,] = ${sample_size}\n",
    "    write.table(n, \"${_output[3]}\", sep = \"\\t\", quote = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ed1cd4-d1bc-4d31-8351-2a1267c328d9",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[CAFEH]\n",
    "input: output_from([\"LD\",\"Sumstat\"])\n",
    "output: f'{cwd}/{_input[0]:bn}/cafeh.beta.results',f'{cwd}/{_input[0]:bn}/cafeh.beta.model'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container, volumes = [f'{_input:ad}:{_input:ad}']\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    from cafeh.cafeh_summary import fit_cafeh_summary, fit_cafeh_z\n",
    "    from cafeh.cafeh_genotype import fit_cafeh_genotype\n",
    "    \n",
    "    from cafeh.model_queries import *\n",
    "    LD_df = pd.read_csv(${_input[1]:r},sep='\\t',index_col=0)\n",
    "    beta_df = pd.read_csv(${_input[2]:r},sep='\\t',index_col=0)\n",
    "    # Drop specify condition\n",
    "    if ${drop_study[0]} > -1:\n",
    "        beta_df = beta_df.drop(beta_df.iloc[[${','.join(drop_study)}],].index)\n",
    "    # Drop if SNPs are empty in all study, then drop study that have NA values\n",
    "    beta_df = beta_df.dropna( 1, \"all\").dropna()\n",
    "    LD_df = LD_df.dropna(1, \"all\").dropna()\n",
    "    LD_df = LD_df.loc[list(set(beta_df.columns) & set(LD_df.index)),list(set(beta_df.columns) & set(LD_df.index))]\n",
    "    beta_df = beta_df[list(set(beta_df.columns) & set(LD_df.index))]\n",
    "    \n",
    "    stderr_df = pd.read_csv(${_input[3]:r},sep='\\t',index_col=0).loc[list(set(beta_df.index)),list(set(beta_df.columns) & set(LD_df.index))].dropna()\n",
    "    beta_df = beta_df.loc[list(set(stderr_df.index)),]\n",
    "    n_df = pd.read_csv(${_input[5]:r},sep='\\t',index_col=0).loc[beta_df.index,beta_df.columns]\n",
    "    cafeh = fit_cafeh_summary(LD_df, beta_df, stderr_df, n=n_df)\n",
    "    cafeh.save('${_output[1]}', save_data=True)\n",
    "    variant_report = summary_table(cafeh)\n",
    "    variant_report.to_csv(${_output[0]:r}, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8832af-2d3c-435e-903f-e4b1263c68b5",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
