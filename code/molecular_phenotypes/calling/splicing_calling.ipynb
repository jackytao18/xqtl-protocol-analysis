{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binding-ottawa",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Quantifying alternative splicing from RNA-seq data\n",
    "\n",
    "This pipeline implements our pipeline to call alternative splicing events from RNA-seq data, using [`leafcutter`](https://www.nature.com/articles/s41588-017-0004-9) and [`psichomics`](https://academic.oup.com/nar/article/47/2/e7/5114259) to call the RNA-seq data from original `fastq.gz` data. It implements the GTEx pipeline for GTEx/TOPMed project. Please refer to [this page](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md) for detail. The choice of pipeline modules in this project is supported by internal (unpublished) benchmarks from GTEx group.\n",
    "\n",
    "**Various reference data needs to be prepared before using this workflow**. [Here we provide a module](https://cumc.github.io/xqtl-pipeline/code/data_preprocessing/reference_data.html) to download and prepare the reference data. \n",
    "\n",
    "The product of this workflow can be used in generating phenotype tables using /molecular_phenotyles/QC/splicing_normalization.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-indie",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Methods overview\n",
    "\n",
    "There are many types of alternative splicing events. See [Wang et al (2008)](https://pubmed.ncbi.nlm.nih.gov/18978772/) and [Park et al (2018)](https://pubmed.ncbi.nlm.nih.gov/29304370/) for an illustration on different events and how splicings are controlled. We will apply two methods to quantify alternative splicing:\n",
    "\n",
    "1. [`psichomics`](https://academic.oup.com/nar/article/47/2/e7/5114259) that quantifies each specific event. In particular the exon skipping event which is used also in GTEx sQTL analysis.\n",
    "2. [`leafcutter`](https://www.nature.com/articles/s41588-017-0004-9) to quantify the usage of alternatively excised introns. This collectively captures skipped exons, 5’ and 3’ alternative splice site usage and other complex events. The method was previously applied to ROSMAP data as part of the Brain xQTL version 2.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-bathroom",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "Both leafcutter and psichomics section, a meta-data file, white space delimited, containing 4 columns: sample ID, RNA strandness and path to the BAM files input for leafcutter section and to SJ.out.tab files for psichomics section:\n",
    "\n",
    "```\n",
    "sample_id       strand          bam_list                                SJ_list\n",
    "sample_1        rf              sample_1.Aligned.sortedByCoord.out.bam  sample_1.SJ.out.tab\n",
    "sample_2        fr              sample_2.Aligned.sortedByCoord.out.bam  sample_2.SJ.out.tab\n",
    "sample_3        strand_missing  sample_3.Aligned.sortedByCoord.out.bam  sample_3.SJ.out.tab\n",
    "```\n",
    "\n",
    "If only one type of input files is prepared, one of the bam_list column and SJ_list column can be left empty.\n",
    "\n",
    "### `leafcutter`\n",
    "\n",
    "The bam files can be generated by `the STAR_align` workflow from our RNA_calling.ipynb module. \n",
    "\n",
    "All the BAM files should be available under specified folder (default assumes the same folder as where the meta-data file is).\n",
    "\n",
    "If intend to blacklist some chromosomes and not analyze it, add one text file named black_list.txt with one chromosome name per line in the same directory of the meta-data file.\n",
    "\n",
    "\n",
    "### `psichomics`\n",
    "\n",
    "The SJ.out.tab files can be generated by `the STAR_align` workflow from our RNA_calling.ipynb module. \n",
    "\n",
    "All the SJ.out.tab files should be available under specified folder (default assumes the same folder as where the meta-data file is).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-costs",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "### `leafcutter`\n",
    "\n",
    "`{sample_list}` below refers to the name of the meta-data file input.\n",
    "\n",
    "Main output include: \n",
    "\n",
    "- `{sample_list}_intron_usage_perind.counts.gz` file with row id in format: \"chromosome:intron_start:intron_end:cluster_id\", column labeled as input sample names and each type of intron usage ratio under each sample (i.e. #particular intron in a sample / #total introns classified in the same cluster in a sample) in each cells. \n",
    "- `{sample_list}_intron_usage_perind_numers.counts.gz` file with the same row and column label but the count of each intron in each cells.\n",
    "\n",
    "### `psichomics`\n",
    "\n",
    "- `psi_raw_data.tsv` A dataframe of PSI values (quantification of the alternative splicing events) with first column splicing event identifier (for instance, SE_1_-_2125078_2124414_2124284_2121220_C1orf86) is composed of:\n",
    "\n",
    "                   Event type (SE stands for skipped exon)\n",
    "                   Chromosome (1)\n",
    "                   Strand (-)\n",
    "                   Relevant coordinates depending on event type (in this case, the first constitutive exon’s end, the                            alternative exon’ start and end and the second constitutive exon’s start)\n",
    "                   Associated gene (C1orf86)\n",
    "\n",
    "| Splicing Event Type | Abbreviation | [Coordinates](https://bioconductor.org/packages/release/bioc/manuals/psichomics/man/psichomics.pdf) |\n",
    "| --- | --- | --- |\n",
    "| Skipped Exon | SE | constitutive exon 1 end, alternative exon (start and end) and constitutive exon 2 start |\n",
    "| Mutually exclusive exon | MXE | constitutive exon 1 end, alternative exon 1 and 2 (start and end) and constitutive exon 2 start |\n",
    "| Alternative 5' splice site | A5SS | constitutive exon 1 end, alternative exon 1 end and constitutive exon 2 start |\n",
    "| Alternative 3' splice site | A3SS | constitutive exon 1 end, alternative exon 1 start and constitutive exon 2 start |\n",
    "| Alternative first exon | AFE | constitutive exon 1 end, alternative exon 1 end and constitutive exon 2 start |\n",
    "| Alternative last exon | ALE | constitutive exon 1 end, alternative exon 1 start and constitutive exon 2 start |\n",
    "| Alternative first exon (exon-centered - less reliable) | AFE_exon | constitutive exon 1 end, alternative exon 1 end and constitutive exon 2 start |\n",
    "| Alternative last exon (exon-centered - less reliable) | ALE_exon | constitutive exon 1 end, alternative exon 1 start and constitutive exon 2 start |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-lighting",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dcd3e5-819c-4835-b4a0-a83d0ef3ad2c",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "A minimal working example is uploaded in the [google drive](https://drive.google.com/drive/folders/1lpcx3eKG2UpauntLUuJ6bMBjHyIhWW_R). It contains example inputs for leafcutter/psichomics, two spliing annotations for psichomics, the meta-data file list, and a example of blacklist chromosome file for leafcutter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-recruitment",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### For `leafcutter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-craft",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/splicing_calling.ipynb leafcutter \\\n",
    "    --cwd leafcutter_output/ \\\n",
    "    --samples sample_bam.list \\\n",
    "    --container containers/leafcutter.sif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-handbook",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### For `psichomics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-stocks",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run splicing_calling.ipynb psichomics \\\n",
    "    --cwd psidata/output/ \\\n",
    "    --samples psidata/sample_SJ.list \\\n",
    "    --splicing_annotation hg38_suppa.rds \\\n",
    "    --container container/psichomics.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-member",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "mineral-motorcycle",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run splicing_calling.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  leafcutter\n",
      "  psichomics\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        The output directory for generated files.\n",
      "  --samples VAL (as path, required)\n",
      "                        Sample meta data list\n",
      "  --data-dir  path(f\"{samples:d}\")\n",
      "\n",
      "                        Raw data directory, default to the same directory as\n",
      "                        sample list\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  leafcutter_1:\n",
      "    Workflow Options:\n",
      "      --anchor-len 8 (as int)\n",
      "                        anchor length (default 8)\n",
      "      --min-intron-len 50 (as int)\n",
      "                        minimum intron length to be analyzed (default 50)\n",
      "      --max-intron-len 500000 (as int)\n",
      "                        maximum intron length to be analyzed (default 500000)\n",
      "  leafcutter_2:\n",
      "    Workflow Options:\n",
      "      --min-clu-reads 50 (as int)\n",
      "                        minimum reads in a cluster (default 50 reads)\n",
      "      --max-intron-len 500000 (as int)\n",
      "                        maximum intron length to be analyzed (default 500000)\n",
      "  psichomics_1:\n",
      "  psichomics_2:\n"
     ]
    }
   ],
   "source": [
    "sos run splicing_calling.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-ontario",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "australian-declaration",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files. \n",
    "parameter: cwd = path(\"output\")\n",
    "# Sample meta data list\n",
    "parameter: samples = path\n",
    "# Raw data directory, default to the same directory as sample list\n",
    "parameter: data_dir = path(f\"{samples:d}\")\n",
    "# splicing annotation for psichomics\n",
    "parameter: splicing_annotation = \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')\n",
    "\n",
    "def get_samples(fn, dr):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    samples = pd.read_csv(fn, sep='\\t')\n",
    "    names = []\n",
    "    strandness = []\n",
    "    bam_list = []\n",
    "    bam_files = []\n",
    "    SJtab_list = []\n",
    "    SJtab_files = []\n",
    "    \n",
    "    samples = samples.fillna(\"NA\")\n",
    "    names = samples['sample_id'].tolist()\n",
    "    strandness = samples['strand'].tolist()\n",
    "    bam_list = samples['coord_bam_list'].tolist()\n",
    "    SJtab_list = samples['SJ_list'].tolist()\n",
    "    \n",
    "    if ((len(bam_list) == sum(x == \"NA\" for x in bam_list)) & (len(SJtab_list) == sum(x == \"NA\" for x in SJtab_list))):\n",
    "        raise ValueError(\"At least one type of input should be ready\")\n",
    "        \n",
    "    for j in range(len(strandness)):\n",
    "        # for regtools command usage, replace 0 = unstranded/XS, 1 = first-strand/RF, 2 = second-strand/FR\n",
    "        if strandness[j] == 'rf':\n",
    "            strandness[j] = 1\n",
    "        if strandness[j] == 'fr':\n",
    "            strandness[j] = 2\n",
    "        if strandness[j] == 'strand_missing':\n",
    "            strandness[j] = 0\n",
    "            \n",
    "    if (len(bam_list) != 0) & (len(bam_list) != sum(x == \"NA\" for x in bam_list)):\n",
    "        for y in bam_list:\n",
    "            y = os.path.join(dr, y)\n",
    "            if not os.path.isfile(y):\n",
    "                raise ValueError(f\"File {y} does not exist\")\n",
    "            bam_files.append(y)\n",
    "        \n",
    "    if len(bam_list) != len(set(bam_list)):\n",
    "        raise ValueError(\"Duplicated files are found (but should not be allowed) in BAM file list\")\n",
    "    \n",
    "    if (len(SJtab_list) != 0) & (len(SJtab_list) != sum(x == \"NA\" for x in SJtab_list)):\n",
    "        for y in SJtab_list:\n",
    "            y = os.path.join(dr, y)\n",
    "            if not os.path.isfile(y):\n",
    "                raise ValueError(f\"File {y} does not exist\")\n",
    "            SJtab_files.append(y)\n",
    "        \n",
    "    if len(SJtab_list) != len(set(SJtab_list)):\n",
    "        raise ValueError(\"Duplicated files are found (but should not be allowed) in SJ.tab file list\")\n",
    "        \n",
    "    return names, strandness, bam_files, SJtab_files\n",
    "\n",
    "sample_id, strandness, bam_data, SJtab_data = get_samples(samples, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-leisure",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `leafcutter`\n",
    "\n",
    "Documentation: [`leafcutter`](https://davidaknowles.github.io/leafcutter/index.html). The choices of regtool parameters are [discussed here](https://github.com/davidaknowles/leafcutter/issues/127).\n",
    "\n",
    "### Other clustering options:\n",
    "\n",
    "*   \"-q\", \"--quiet\" : don't print status messages to stdout, default=True.\n",
    "\n",
    "*   \"-p\", \"--mincluratio\" : minimum fraction of reads in a cluster that support a junction, default 0.001. \n",
    "\n",
    "*   \"-c\", \"--cluster\" : refined cluster file when clusters are already made, default = None.\n",
    "\n",
    "*   \"-k\", \"--nochromcheck\" : Don't check that the chromosomes are well formated e.g. chr1, chr2, ..., or 1, 2, ..., default = False.\n",
    "\n",
    "*    \"-C\", \"--includeconst\" : also include constitutive introns, default = False.\n",
    "\n",
    "The default parameter we used are:\n",
    "\n",
    "`--min_clu_ratio 0.001 --max_intron_len 500000 --min_clu_reads 30`\n",
    "\n",
    "These parameter is based on [GTEX's sQTL discovery pipeline (Section 3.4.3) ](https://www.science.org/action/downloadSupplement?doi=10.1126%2Fscience.aaz1776&file=aaz1776_aguet_sm.pdf)\n",
    "\n",
    "### Things to keep in mind:\n",
    "\n",
    "* If .bam.bai index files of the .bam input are ready before using leafCutter, it can be placed in the same directory with input .bam files and the \"samtools index ${_input}\" line can be skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aerial-temperature",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_1]\n",
    "# anchor length (default 8)\n",
    "parameter: anchor_len = 8\n",
    "# minimum intron length to be analyzed (default 50)\n",
    "parameter: min_intron_len = 50\n",
    "# maximum intron length to be analyzed (default 500000)\n",
    "parameter: max_intron_len = 500000\n",
    "input: bam_data, group_by = 1, group_with = \"strandness\"\n",
    "output: f'{cwd}/{_input:bn}.junc' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    samtools index ${_input}\n",
    "    regtools junctions extract -a ${anchor_len} -m ${min_intron_len} -M ${max_intron_len} -s ${_strandness} ${_input} -o ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "going-history",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_2]\n",
    "# minimum reads in a cluster (default 50 reads)\n",
    "parameter: min_clu_reads = 30 \n",
    "# maximum intron length to be analyzed (default 500000)\n",
    "parameter: max_intron_len = 500000 \n",
    "# minimum fraction of reads in a cluster that support a junction (default 0.001)\n",
    "parameter: min_clu_ratio = 0.001\n",
    "input: group_by = 'all'\n",
    "output: f'{cwd}/{samples:bn}_intron_usage_perind.counts.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    rm -f ${_output:nn}.junc\n",
    "    for i in ${_input:r}; do\n",
    "    echo $i >> ${_output:nn}.junc ; done\n",
    "    python /opt/leafcutter/clustering/leafcutter_cluster_regtools.py -j ${_output:nn}.junc -o ${f'{_output:bnn}'.replace(\"_perind\",\"\")} -m ${min_clu_reads} -l ${max_intron_len} -r ${cwd} -p ${min_clu_ratio}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-reception",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `psichomics`\n",
    "\n",
    "Documentation: [`psichomics`](http://bioconductor.org/packages/release/bioc/html/psichomics.html)\n",
    "\n",
    "### Other options\n",
    "\n",
    "quantifySplicing( annotation,\n",
    "                  junctionQuant,\n",
    "                  eventType = c(\"SE\", \"MXE\", \"ALE\", \"AFE\", \"A3SS\", \"A5SS\"),\n",
    "                  minReads = 10,\n",
    "                  genes = NULL\n",
    ")\n",
    "\n",
    "In function quantifySplicing, arguments eventType (Character: splicing event types to quantify), minReads (Integer: values whose number of total supporting read counts is below minReads are returned as NA) and genes (Character: gene symbols for which to quantify splicing events. If NULL, events from all genes are quantified.) can be specified. Usage and default values are shown above.\n",
    "\n",
    "### Alternative Splicing Annotation Information\n",
    "\n",
    "Two alternative splicing annotations will be provided in this pipeline which can be download [here](https://drive.google.com/drive/folders/1lpcx3eKG2UpauntLUuJ6bMBjHyIhWW_R). The hg38_suppa.rds is created Via SUPPA using the gtf file of the xqtl-pipeline, and the modified_psichomics_hg38_splicing_annotation.rds is modified from the default Human hg38 (2018-04-30) annotation provided by psichomics package. Description of the database can be found in the Alternative splicing annotation section in the [MATERIALS AND METHODS](https://academic.oup.com/nar/article/47/2/e7/5114259?login=true#130023625) part. Gene names of the original annotation are replaced by Ensembl ids for format unifying. The Ensembl IDs used in modifiction are matched from the gtf file, HGNC database, SUPPA and VASTTOOL records within the original annotation.\n",
    "\n",
    "Theoretically the annotation created using the gtf file only will give results more consistent with other part of the pipeline. The annotation modified from psichomics original hg38 annotation can identify more events since it was build based on information maximizing principle, however there will be risk of containing outdated information too. \n",
    "\n",
    "For details of generation method of the gtf file and the two splicing annotations, please check the GFF3 to GTF formatting, Generation of SUPPA annotation for psichomics, and Modification of psichomics default Hg38 splicing annotation sections in [reference_data.ipynb](https://github.com/cumc/xqtl-pipeline/blob/main/code/data_preprocessing/reference_data.ipynb).\n",
    "\n",
    "### Things to keep in mind:\n",
    "\n",
    "* The script below allows to run prepareJunctionQuant() function from psichomics package on different input directories, however the prepareJunctionQuant() function will generate one psichomics_junctions.txt in each input directory. The psichomics_junctions.txt files generated in input directories are recommanded to be deleted before rerun of the psichimics_1 step since there probabaly be an overwrite conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1af0c3e1-4ad4-42ff-8f36-425db003e59e",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[psichomics_1]\n",
    "input: SJtab_data\n",
    "output: f'{cwd}/psi_raw_data.tsv'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    library(\"psichomics\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"purrr\")\n",
    "  \n",
    "    files = list()\n",
    "  \n",
    "    for (f in c(${_input:ar,})){\n",
    "      filename = gsub(\"^.*/\", \"\", f)\n",
    "      directory = gsub(filename, \"\", f)\n",
    "      if (length(files[[directory]]) == 0){\n",
    "        files[[directory]] = filename\n",
    "        } else {\n",
    "      files[[directory]] = append(files[[directory]], filename)\n",
    "      }\n",
    "    }\n",
    "  \n",
    "    if (length(files) == 1) {\n",
    "      setwd(names(files)[1])\n",
    "      res = prepareJunctionQuant(files[[1]])\n",
    "    } else {\n",
    "    res = list()\n",
    "    for (i in 1:(length(files))) {\n",
    "        d = names(files[i])\n",
    "        setwd(d)\n",
    "        res[[d]] = prepareJunctionQuant(files[[d]])\n",
    "        }\n",
    "    res = res %>% reduce(full_join, by = \"Junction ID\")\n",
    "    }\n",
    "    \n",
    "    res[is.na(res)] <- 0\n",
    "  \n",
    "    write.table(res, file='${cwd}/psichomics_junctions.txt', quote=FALSE, sep='\\t', row.names=FALSE)\n",
    "    \n",
    "    data <- loadLocalFiles(\"${cwd}\")\n",
    "    junctionQuant <- data[[1]]$`Junction quantification`\n",
    "    annotation = readRDS(\"${splicing_annotation}\")\n",
    "    psi <- quantifySplicing(annotation, junctionQuant)\n",
    "    write.table(psi, file='${cwd}/psi_raw_data.tsv', quote=FALSE, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     ""
    ],
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
